{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import findspark\r\n",
    "import dotenv\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dotenv.load_dotenv(dotenv.find_dotenv())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "findspark.init=os.getenv('Spark')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "sc = SparkSession.builder.master('local[*]').getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df_spark = sc.read.csv(\"../Backup/some_file.csv\", inferSchema=True, header=True, sep=';')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df_spark.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- data_dado_inserido: string (nullable = true)\n",
      " |-- papel: string (nullable = true)\n",
      " |-- tipo: string (nullable = true)\n",
      " |-- empresa: string (nullable = true)\n",
      " |-- setor: string (nullable = true)\n",
      " |-- cotacao: string (nullable = true)\n",
      " |-- data_ult_cotacao: string (nullable = true)\n",
      " |-- min_52_sem: string (nullable = true)\n",
      " |-- max_52_sem: string (nullable = true)\n",
      " |-- vol_med_2m: string (nullable = true)\n",
      " |-- valor_mercado: string (nullable = true)\n",
      " |-- valor_firma: string (nullable = true)\n",
      " |-- ult_balanco_pro: string (nullable = true)\n",
      " |-- nr_acoes: long (nullable = true)\n",
      " |-- os_dia: double (nullable = true)\n",
      " |-- pl: double (nullable = true)\n",
      " |-- lpa: double (nullable = true)\n",
      " |-- pvp: double (nullable = true)\n",
      " |-- vpa: double (nullable = true)\n",
      " |-- p_ebit: double (nullable = true)\n",
      " |-- marg_bruta: double (nullable = true)\n",
      " |-- psr: double (nullable = true)\n",
      " |-- marg_ebit: double (nullable = true)\n",
      " |-- p_ativos: double (nullable = true)\n",
      " |-- marg_liquida: double (nullable = true)\n",
      " |-- p_cap_giro: double (nullable = true)\n",
      " |-- ebit_ativo: double (nullable = true)\n",
      " |-- p_ativo_circ_liq: double (nullable = true)\n",
      " |-- roic: double (nullable = true)\n",
      " |-- div_yield: double (nullable = true)\n",
      " |-- roe: double (nullable = true)\n",
      " |-- ev_ebitda: double (nullable = true)\n",
      " |-- liquidez_corr: double (nullable = true)\n",
      " |-- ev_ebit: double (nullable = true)\n",
      " |-- cres_rec_5a: double (nullable = true)\n",
      " |-- ativo: string (nullable = true)\n",
      " |-- disponibilidades: string (nullable = true)\n",
      " |-- ativo_circulante: string (nullable = true)\n",
      " |-- div_bruta: string (nullable = true)\n",
      " |-- div_liquida: string (nullable = true)\n",
      " |-- patr_liquido: string (nullable = true)\n",
      " |-- lucro_liquido_12m: string (nullable = true)\n",
      " |-- lucro_liquido_3m: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark.sql import SparkSession, Row\r\n",
    "from pyspark.sql.functions import col,when\r\n",
    "import pydeequ"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Please set env variable SPARK_VERSION\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark = (SparkSession\r\n",
    "    .builder\r\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\r\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\r\n",
    "    .getOrCreate())\r\n",
    "\r\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df1 = (spark\r\n",
    "       .read\r\n",
    "       .format(\"csv\")\r\n",
    "       .option(\"header\", \"true\")\r\n",
    "       .option(\"encoding\", \"ISO-8859-1\")\r\n",
    "       .load(\"dados-go-1.csv\", sep = ';'))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21704/3125483129.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m df1 = (spark\n\u001b[0m\u001b[0;32m      2\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ISO-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "afc191c8ee3715319b22903a0fd4797943b739b16a492493655bb7270ab7e685"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}